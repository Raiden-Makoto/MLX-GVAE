{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78d5a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501cf556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GINEConv, global_add_pool\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cad70d",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d21c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3e0f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QM9(root='./QM9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18dd227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "g = torch.Generator().manual_seed(67)\n",
    "sample_size = int(0.1 * len(dataset))\n",
    "dataset_67, _ = random_split(dataset, [sample_size, len(dataset) - sample_size], generator=g)\n",
    "\n",
    "num_train = int(0.8 * len(dataset_67))\n",
    "num_val = int(0.1 * len(dataset_67))\n",
    "train_set, val_set, test_set = random_split(\n",
    "    dataset_67, [num_train, num_val, len(dataset_67) - num_train - num_val], generator=g\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78122806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 10466\n",
      "Validation set size: 1308\n",
      "Test set size: 1309\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_set,   batch_size=64)\n",
    "test_loader  = DataLoader(test_set,  batch_size=64)\n",
    "print(\"Train set size:\", len(train_set))\n",
    "print(\"Validation set size:\", len(val_set))\n",
    "print(\"Test set size:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d650bd5f",
   "metadata": {},
   "source": [
    "## Encoder Architecture\n",
    "The **Encoder** consists of:\n",
    "- Message-passing layers (e.g., GCNConv, GINEConv) to compute node embeddings.\n",
    "- Global readout (e.g., `global_add_pool`) to obtain graph representation $h$\n",
    "- Two separate MLPs mapping $h$ to latent mean $\\mu$ and log-variance log $\\sigma^2$\n",
    "\n",
    "**Reparameterization**    \n",
    "Sample $z = \\mu + \\exp(\\frac{1}{2} \\sigma^2) \\odot \\varepsilon$ where $\\varepsilon \\sim N(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cc6c6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, latent_dim, edge_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = GINEConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_channels, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ),\n",
    "            edge_dim=edge_dim\n",
    "        )\n",
    "        self.conv2 = GINEConv(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            ),\n",
    "            edge_dim=edge_dim\n",
    "        )\n",
    "        self.readout = global_add_pool\n",
    "        self.lin_mu  = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.lin_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = torch.relu(x)\n",
    "        h = self.readout(x, batch)\n",
    "        mu = self.lin_mu(h)\n",
    "        logvar = self.lin_logvar(h)\n",
    "        return mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af83c2de",
   "metadata": {},
   "source": [
    "## Decoder Architecture\n",
    "Condition on $z$ to reconstruct adjacency and node features. For example:\n",
    "- Edge-wise MLP: For each potential node pair, predict bond existence/type.\n",
    "- Autoregressive (e.g., graph sequential decoding).\n",
    "- Graph deconvolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b45145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim,\n",
    "        hidden_dim,\n",
    "        num_node_types,\n",
    "        num_edge_types,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.node_mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_node_types),\n",
    "        )\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(2 * latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_edge_types),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, edge_index, batch):\n",
    "        # z: [num_graphs_in_batch, latent_dim]\n",
    "        # batch: [num_nodes] mapping each node to its graph index\n",
    "        # Broadcast z to node-level using batch\n",
    "        z_nodes = z[batch]               # -> [num_nodes, latent_dim]\n",
    "        node_logits = self.node_mlp(z_nodes)\n",
    "\n",
    "        src, dst   = edge_index\n",
    "        edge_inputs = torch.cat([z_nodes[src], z_nodes[dst]], dim=-1)\n",
    "        edge_logits = self.edge_mlp(edge_inputs)\n",
    "\n",
    "        return node_logits, edge_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21cb81",
   "metadata": {},
   "source": [
    "## Graph VAE Setup\n",
    "**Loss**: Reconstruction Loss: Cross-entropy for discrete features (atom types, bond types)    \n",
    "**KL Divergence**: $\\text{KL} = -\\frac{1}{2}\\sum_{i=1}^d(1 + \\log \\sigma_i^2 - \\mu_i^2 - \\sigma_i^2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e80f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import VGAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032e7af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (conv1): GINEConv(nn=Sequential(\n",
       "    (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  ))\n",
       "  (conv2): GINEConv(nn=Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  ))\n",
       "  (lin_mu): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (lin_logvar): Linear(in_features=128, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = Encoder(\n",
    "    in_channels=dataset.num_node_features,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    edge_dim=dataset.num_edge_features\n",
    ")\n",
    "E.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6c6401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (node_mlp): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       "  (edge_mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = Decoder(\n",
    "    latent_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_node_types=5, # C, N, O, F, H\n",
    "    num_edge_types=4 # single, double, triple, aromatic\n",
    ")\n",
    "D.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4228ada0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): GINEConv(nn=Sequential(\n",
       "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    ))\n",
       "    (conv2): GINEConv(nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    ))\n",
       "    (lin_mu): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (lin_logvar): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGAE(encoder=E, decoder=D)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07acb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer =  Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2ab2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_node_loss = nn.CrossEntropyLoss()\n",
    "recon_edge_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f1569f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(data, node_logits, edge_logits, mu, logvar):\n",
    "    recon_node_loss = nn.CrossEntropyLoss()(node_logits, data.x.argmax(dim=1))\n",
    "    recon_edge_loss = nn.CrossEntropyLoss()(edge_logits, data.edge_attr.argmax(dim=1))\n",
    "    kl = model.kl_loss(mu, logvar)\n",
    "    return recon_node_loss + recon_edge_loss + 1e-4 * kl # 1e-4 is a small constant to prevent KL loss from dominating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f8bf6",
   "metadata": {},
   "source": [
    "## Model Training (10 Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b66f3da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 — Loss: 238.3771\n",
      "Epoch 02 — Loss: 0.2649\n",
      "Epoch 03 — Loss: 0.2523\n",
      "Epoch 04 — Loss: 0.2423\n",
      "Epoch 05 — Loss: 0.2344\n",
      "Epoch 06 — Loss: 0.2300\n",
      "Epoch 07 — Loss: 0.2272\n",
      "Epoch 08 — Loss: 0.2248\n",
      "Epoch 09 — Loss: 0.2228\n",
      "Epoch 10 — Loss: 0.2220\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Encode: get mu and logvar\n",
    "        mu, logvar = model.encoder(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "\n",
    "        # 2. Reparameterize to obtain z\n",
    "        z = model.reparametrize(mu, logvar)\n",
    "\n",
    "        # 3. Decode: predict node and edge logits\n",
    "        node_logits, edge_logits = model.decoder(z, data.edge_index, data.batch)\n",
    "\n",
    "        # 4. Compute loss\n",
    "        loss = loss_fn(data, node_logits, edge_logits, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch:02d} — Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3424f098",
   "metadata": {},
   "source": [
    "Less than $1\\%$ of the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2708c1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = {\n",
    "    \"epoch\": epoch,\n",
    "    \"encoder\": E.state_dict(),\n",
    "    \"decoder\": D.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"train_args\": {\"lr\": 1e-3, \"batch_size\": 64},\n",
    "}\n",
    "torch.save(ckpt, \"checkpoints/qvae_epoch_{:03d}.pt\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8244eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(ckpt, \"checkpoints/qvae_best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8cdeb2",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03812d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"checkpoints/qvae_best.pt\", map_location=device)\n",
    "E.load_state_dict(ckpt[\"encoder\"])\n",
    "D.load_state_dict(ckpt[\"decoder\"])\n",
    "optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
    "start_epoch = ckpt.get(\"epoch\", 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09d2d67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv1): GINEConv(nn=Sequential(\n",
       "      (0): Linear(in_features=11, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    ))\n",
       "    (conv2): GINEConv(nn=Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    ))\n",
       "    (lin_mu): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (lin_logvar): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (node_mlp): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=5, bias=True)\n",
       "    )\n",
       "    (edge_mlp): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VGAE(encoder=E, decoder=D)\n",
    "vae.to(device)\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdd03424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_molecules(model, num_samples, edge_index, batch, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 1. Sample latent vectors\n",
    "        z = torch.randn(num_samples, model.encoder.lin_mu.out_features).to(device)\n",
    "\n",
    "        # 2. Decode to get logits\n",
    "        node_logits, edge_logits = model.decoder(z, edge_index, batch)\n",
    "\n",
    "        # 3. Discretize\n",
    "        atom_types = node_logits.argmax(dim=-1)\n",
    "        bond_types = edge_logits.argmax(dim=-1)\n",
    "\n",
    "    return atom_types, bond_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fe70da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generation_graph(num_samples: int, max_nodes: int, device):\n",
    "    # 1. Build batch: [0,0,…,0,1,1,…,1,…,num_samples-1,…]\n",
    "    batch = torch.arange(num_samples, device=device).unsqueeze(1).repeat(1, max_nodes).view(-1)  # shape [num_samples * max_nodes]\n",
    "\n",
    "    # 2. Build node indices for each sample\n",
    "    node_offsets = torch.arange(num_samples, device=device) * max_nodes\n",
    "    node_indices = (node_offsets.unsqueeze(1) + torch.arange(max_nodes, device=device)).view(-1)  # shape [num_samples * max_nodes]\n",
    "\n",
    "    # 3. Create all possible directed edges (excluding self-loops)\n",
    "    src = node_indices.unsqueeze(1).repeat(1, max_nodes).view(-1)  # repeat each node max_nodes times\n",
    "    dst = node_indices.repeat(max_nodes)                          # tile the entire list max_nodes times\n",
    "\n",
    "    # 4. Mask out self-loops\n",
    "    mask = src != dst\n",
    "    edge_index = torch.stack([src[mask], dst[mask]], dim=0)       # shape [2, num_edges]\n",
    "\n",
    "    return batch, edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4411d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_batch, gen_edge_index = make_generation_graph(10, 9, device)\n",
    "atom_types, bond_types = sample_molecules(vae, 10, gen_edge_index, gen_batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a3f3a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "element_list = ['H','C','N','O','F']\n",
    "bond_type_list = [\n",
    "    Chem.rdchem.BondType.SINGLE,\n",
    "    Chem.rdchem.BondType.DOUBLE,\n",
    "    Chem.rdchem.BondType.TRIPLE,\n",
    "    Chem.rdchem.BondType.AROMATIC\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab9e3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules = []\n",
    "start = 0\n",
    "for i in range(10):\n",
    "    # Select nodes for this molecule\n",
    "    node_mask = gen_batch == i\n",
    "    n_nodes = node_mask.sum().item()\n",
    "\n",
    "    # Build node features: one-hot from atom_types\n",
    "    ats = atom_types[start:start+n_nodes]\n",
    "    x = torch.nn.functional.one_hot(ats, num_classes=5).float()\n",
    "\n",
    "    # Select edges for this molecule\n",
    "    edge_mask = (gen_batch[gen_edge_index[0]] == i) & (gen_batch[gen_edge_index[1]] == i)\n",
    "    ei = gen_edge_index[:, edge_mask]\n",
    "    bs = bond_types[edge_mask]\n",
    "    edge_attr = torch.nn.functional.one_hot(bs, num_classes=4).float()\n",
    "\n",
    "    data = Data(x=x, edge_index=ei, edge_attr=edge_attr)\n",
    "    molecules.append(data)\n",
    "    start += n_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e44190fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rdkit(data):\n",
    "    mol = Chem.RWMol()\n",
    "    # Add atoms\n",
    "    for at in data.x.argmax(dim=1).tolist():\n",
    "        mol.AddAtom(Chem.Atom(element_list[at]))\n",
    "    # Add bonds\n",
    "    src, dst = data.edge_index\n",
    "    for u, v, bt in zip(src.tolist(), dst.tolist(), data.edge_attr.argmax(dim=1).tolist()):\n",
    "        # Avoid duplicates in undirected graph\n",
    "        if u < v:\n",
    "            mol.AddBond(u, v, bond_type_list[bt])\n",
    "    Chem.SanitizeMol(mol)\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be0000fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[09:00:13] Explicit valence for atom # 0 C, 16, is greater than permitted\n"
     ]
    },
    {
     "ename": "AtomValenceException",
     "evalue": "Explicit valence for atom # 0 C, 16, is greater than permitted",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAtomValenceException\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rdkit_mols = \u001b[43m[\u001b[49m\u001b[43mto_rdkit\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmolecules\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m rdkit_mols = [\u001b[43mto_rdkit\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m molecules]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mto_rdkit\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m u < v:\n\u001b[32m     11\u001b[39m         mol.AddBond(u, v, bond_type_list[bt])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mChem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSanitizeMol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m mol\n",
      "\u001b[31mAtomValenceException\u001b[39m: Explicit valence for atom # 0 C, 16, is greater than permitted"
     ]
    }
   ],
   "source": [
    "rdkit_mols = [to_rdkit(d) for d in molecules]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c854a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qvae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
